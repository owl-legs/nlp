{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:47:55.158854Z",
     "start_time": "2025-03-07T04:47:55.151064Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/alexrotondo/nlp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:48:00.170671Z",
     "start_time": "2025-03-07T04:47:59.676699Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  artist                   song                                        link  \\\n0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n\n                                                text  \n0  Look at her face, it's a wonderful face  \\nAnd...  \n1  Take it easy with me, please  \\nTouch me gentl...  \n2  I'll never know why I had to go  \\nWhy I had t...  \n3  Making somebody happy is a question of give an...  \n4  Making somebody happy is a question of give an...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>song</th>\n      <th>link</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABBA</td>\n      <td>Ahe's My Kind Of Girl</td>\n      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ABBA</td>\n      <td>Andante, Andante</td>\n      <td>/a/abba/andante+andante_20002708.html</td>\n      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ABBA</td>\n      <td>As Good As New</td>\n      <td>/a/abba/as+good+as+new_20003033.html</td>\n      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ABBA</td>\n      <td>Bang</td>\n      <td>/a/abba/bang_20598415.html</td>\n      <td>Making somebody happy is a question of give an...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ABBA</td>\n      <td>Bang-A-Boomerang</td>\n      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n      <td>Making somebody happy is a question of give an...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "million_songs = pd.read_csv('embeddings/data/spotify_millsongdata.csv')\n",
    "million_songs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:48:02.668835Z",
     "start_time": "2025-03-07T04:48:01.936407Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "million_songs.text.isna().sum() / million_songs.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:48:04.856433Z",
     "start_time": "2025-03-07T04:48:04.822367Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "song_lyrics = million_songs.text.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:48:06.020374Z",
     "start_time": "2025-03-07T04:48:06.014826Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "song_lyrics = [sl.replace(\"\\n\", \"\") for sl in song_lyrics]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:48:06.620580Z",
     "start_time": "2025-03-07T04:48:06.501450Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of empty songs 0.0\n"
     ]
    }
   ],
   "source": [
    "empty_songs = len(list(filter(lambda x: x == \"\" or x.isspace(), song_lyrics)))\n",
    "print(f\"proportion of empty songs {empty_songs / len(song_lyrics)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:48:07.364704Z",
     "start_time": "2025-03-07T04:48:07.331794Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "typing.Optional requires a single type. Got Field(name=None,type=None,default=<dataclasses._MISSING_TYPE object at 0x10b144850>,default_factory=.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mstring\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbag_of_words\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbag_of_words\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m embed_documents\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DocumentConfig\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mstopwords\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_stopwords\n",
      "File \u001B[0;32m~/nlp/embeddings/bag_of_words/bag_of_words.py:7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvocab\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CorpusVocab\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenizer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m lower_text, remove_punctuation, remove_stopwords, tokenize_document\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DocumentConfig\n",
      "File \u001B[0;32m~/nlp/embeddings/utils/vocab.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcollections\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mdataclasses\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m dataclass, field\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DocumentConfig\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01membeddings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenizer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m lower_text, remove_punctuation, tokenize_document\n\u001B[1;32m      8\u001B[0m \u001B[38;5;129m@dataclass\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mCorpusVocab\u001B[39;00m:\n",
      "File \u001B[0;32m~/nlp/embeddings/utils/config.py:13\u001B[0m\n\u001B[1;32m      8\u001B[0m STANDARD_PUNCTUATION \u001B[38;5;241m=\u001B[39m string\u001B[38;5;241m.\u001B[39mpunctuation\n\u001B[1;32m      9\u001B[0m STANDARD_STOPWORDS \u001B[38;5;241m=\u001B[39m get_stopwords()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;129m@dataclass\u001B[39m\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mDocumentConfig\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     lower_text: \u001B[38;5;28mbool\u001B[39m\n\u001B[1;32m     15\u001B[0m     remove_punctuation: \u001B[38;5;28mbool\u001B[39m\n",
      "File \u001B[0;32m~/nlp/embeddings/utils/config.py:18\u001B[0m, in \u001B[0;36mDocumentConfig\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m remove_punctuation: \u001B[38;5;28mbool\u001B[39m\n\u001B[1;32m     16\u001B[0m remove_stopwords: \u001B[38;5;28mbool\u001B[39m\n\u001B[0;32m---> 18\u001B[0m excluded_punctuation: \u001B[43mOptional\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfield\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdefault_factory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     19\u001B[0m stopwords: Optional[field(default_factory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m)]\n\u001B[1;32m     21\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m     23\u001B[0m            lower_text: \u001B[38;5;28mbool\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m            stopwords: Optional[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]]\n\u001B[1;32m     28\u001B[0m            ):\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/typing.py:275\u001B[0m, in \u001B[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# All real errors (not unhashable args) are raised below.\u001B[39;00m\n\u001B[0;32m--> 275\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/typing.py:352\u001B[0m, in \u001B[0;36m_SpecialForm.__getitem__\u001B[0;34m(self, parameters)\u001B[0m\n\u001B[1;32m    350\u001B[0m \u001B[38;5;129m@_tp_cache\u001B[39m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, parameters):\n\u001B[0;32m--> 352\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/typing.py:475\u001B[0m, in \u001B[0;36mOptional\u001B[0;34m(self, parameters)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;129m@_SpecialForm\u001B[39m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mOptional\u001B[39m(\u001B[38;5;28mself\u001B[39m, parameters):\n\u001B[1;32m    471\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optional type.\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m    Optional[X] is equivalent to Union[X, None].\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     arg \u001B[38;5;241m=\u001B[39m \u001B[43m_type_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m requires a single type.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Union[arg, \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m)]\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/typing.py:164\u001B[0m, in \u001B[0;36m_type_check\u001B[0;34m(arg, msg, is_argument)\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arg\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(arg):\n\u001B[0;32m--> 164\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marg\u001B[38;5;132;01m!r:\u001B[39;00m\u001B[38;5;124m.100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arg\n",
      "\u001B[0;31mTypeError\u001B[0m: typing.Optional requires a single type. Got Field(name=None,type=None,default=<dataclasses._MISSING_TYPE object at 0x10b144850>,default_factory=."
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from embeddings.bag_of_words.bag_of_words import embed_documents\n",
    "from embeddings.utils.config import DocumentConfig\n",
    "\n",
    "from stopwords import get_stopwords\n",
    "\n",
    "STANDARD_PUNCTUATION = string.punctuation\n",
    "STANDARD_STOPWORDS = get_stopwords()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:48:08.767299Z",
     "start_time": "2025-03-07T04:48:08.379945Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 9\u001B[0m\n\u001B[1;32m      1\u001B[0m document_config \u001B[38;5;241m=\u001B[39m DocumentConfig\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m      2\u001B[0m     lower_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      3\u001B[0m     exclude_punctuation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m     stopwords\u001B[38;5;241m=\u001B[39mSTANDARD_STOPWORDS\n\u001B[1;32m      7\u001B[0m )\n\u001B[0;32m----> 9\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocument_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msong_lyrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocument_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocument_config\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp/embeddings/bag_of_words/bag_of_words.py:30\u001B[0m, in \u001B[0;36membed_documents\u001B[0;34m(document_list, document_config)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed_documents\u001B[39m(\n\u001B[1;32m     27\u001B[0m     document_list: Optional[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]],\n\u001B[1;32m     28\u001B[0m     document_config: DocumentConfig,\n\u001B[1;32m     29\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m]]:\n\u001B[0;32m---> 30\u001B[0m     corpus_vocab \u001B[38;5;241m=\u001B[39m \u001B[43mCorpusVocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocument_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocument_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocument_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m document \u001B[38;5;129;01min\u001B[39;00m document_list:\n",
      "File \u001B[0;32m~/nlp/embeddings/utils/vocab.py:25\u001B[0m, in \u001B[0;36mCorpusVocab.create\u001B[0;34m(cls, documents, document_config)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m document_config\u001B[38;5;241m.\u001B[39mremove_punctuation:\n\u001B[1;32m     23\u001B[0m     document \u001B[38;5;241m=\u001B[39m remove_punctuation(document\u001B[38;5;241m=\u001B[39mdocument)\n\u001B[0;32m---> 25\u001B[0m document_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mtokenize_document\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocument\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m document_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(document_tokens)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m document_tokens:\n",
      "File \u001B[0;32m~/nlp/embeddings/utils/tokenizer.py:22\u001B[0m, in \u001B[0;36mtokenize_document\u001B[0;34m(document, max_tokens, delimiter)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtokenize_document\u001B[39m(\n\u001B[1;32m     18\u001B[0m         document: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m     19\u001B[0m         max_tokens: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,  \u001B[38;5;66;03m# only considers the first `max_tokens` of the document\u001B[39;00m\n\u001B[1;32m     20\u001B[0m         delimiter: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     21\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m---> 22\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m \u001B[43mdocument\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelimiter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaxsplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m     24\u001B[0m         token\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tokens\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m token\u001B[38;5;241m.\u001B[39misspace()) \u001B[38;5;129;01mor\u001B[39;00m (token \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     27\u001B[0m     ]\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "document_config = DocumentConfig.create(\n",
    "    lower_text=True,\n",
    "    exclude_punctuation=True,\n",
    "    remove_stopwords=True,\n",
    "    excluded_punctuation=STANDARD_PUNCTUATION,\n",
    "    stopwords=STANDARD_STOPWORDS\n",
    ")\n",
    "\n",
    "embeddings = embed_documents(\n",
    "    document_list=song_lyrics,\n",
    "    document_config=document_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-07T04:44:27.411809Z",
     "start_time": "2025-03-07T04:44:27.379212Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
